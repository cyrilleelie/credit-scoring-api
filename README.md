---
title: Credit Scoring API
emoji: ğŸ¤–
colorFrom: green
colorTo: blue
sdk: docker
app_port: 8000
pinned: false
---

# API de Scoring CrÃ©dit & Dashboard de Monitoring

Ce projet a pour objectif de dÃ©ployer un modÃ¨le de Machine Learning de scoring crÃ©dit via une API robuste (FastAPI) et de fournir un dashboard interactif (Streamlit) pour l'analyse et le monitoring en temps rÃ©el. L'ensemble de l'application est conÃ§u pour Ãªtre conteneurisable avec Docker et est supportÃ© par une base de donnÃ©es PostgreSQL.

## ğŸ›ï¸ Architecture

L'application est composÃ©e de trois services principaux conÃ§us pour fonctionner ensemble :

1.  **Base de DonnÃ©es PostgreSQL** : ConteneurisÃ©e avec Docker, elle stocke les donnÃ©es des clients, les utilisateurs de l'application, les logs d'API et les rapports de dÃ©rive.
2.  **API FastAPI** : Sert le modÃ¨le de scoring. Elle expose des endpoints sÃ©curisÃ©s pour l'authentification et la prÃ©diction, et enregistre chaque appel dans la base de donnÃ©es.
3.  **Dashboard Streamlit** : Fournit une interface utilisateur pour interagir avec l'API, visualiser les performances et analyser la dÃ©rive des donnÃ©es.

## ğŸ“‚ Structure du Projet

Le code est organisÃ© en modules fonctionnels pour une meilleure clartÃ© et maintenabilitÃ©.

```
credit-scoring-api/
â”œâ”€â”€ .github/workflows/    # Workflows d'IntÃ©gration Continue (CI)
â”œâ”€â”€ model_artifacts/      # ModÃ¨les entraÃ®nÃ©s (ignorÃ© par Git)
â”œâ”€â”€ src/                  # Code source de l'application
â”‚   â”œâ”€â”€ api/              # Logique de l'API FastAPI
â”‚   â”œâ”€â”€ config/           # Configuration de l'application
â”‚   â”œâ”€â”€ dashboard/        # Logique du Dashboard Streamlit
â”‚   â”œâ”€â”€ database/         # ModÃ¨les de donnÃ©es et connexion BDD
â”‚   â””â”€â”€ scripts/          # Scripts utilitaires (init_db, profiling, etc.)
â”œâ”€â”€ tests/                # Tests automatisÃ©s
â”‚   â”œâ”€â”€ fixtures/         # Petits jeux de donnÃ©es pour les tests
â”‚   â””â”€â”€ test_api.py
â”œâ”€â”€ .env.example          # Fichier d'exemple pour la configuration
â”œâ”€â”€ .gitignore
â”œâ”€â”€ app.py                # Point d'entrÃ©e pour le dashboard Streamlit
â”œâ”€â”€ docker-compose.yml    # Configuration pour lancer la BDD avec Docker
â””â”€â”€ pyproject.toml        # DÃ©pendances et configuration du projet (Poetry)
```

## ğŸš€ Installation et Lancement

Suivez ces Ã©tapes pour lancer l'application en environnement de dÃ©veloppement local.

### 1. PrÃ©requis

* [Git](https://git-scm.com/)
* [Python 3.11+](https://www.python.org/)
* [Poetry](https://python-poetry.org/)
* [Docker](https://www.docker.com/) et Docker Compose

### 2. Cloner le DÃ©pÃ´t

```bash
git clone [https://github.com/cyrilleelie/OC_credit-scoring-api](https://github.com/cyrilleelie/OC_credit-scoring-api)
cd credit-scoring-api
```

### 3. Fichier de Configuration

CrÃ©ez votre fichier de configuration local Ã  partir de l'exemple fourni.

```bash
cp .env.example .env
```
**Action requise :** Ouvrez le fichier `.env` et remplissez les valeurs, notamment les identifiants de la base de donnÃ©es et les chemins vers les fichiers de donnÃ©es si vous les utilisez localement.

### 4. Installer les DÃ©pendances

Ce projet utilise Poetry. Installez toutes les dÃ©pendances nÃ©cessaires :

```bash
poetry install
```

### 5. DÃ©marrer la Base de DonnÃ©es

Lancez le conteneur PostgreSQL en arriÃ¨re-plan avec Docker Compose :

```bash
docker-compose up -d
```

### 6. Initialiser la Base de DonnÃ©es

Ce script crÃ©e le schÃ©ma de la base de donnÃ©es et y charge les donnÃ©es des clients.

**Important :** Les fichiers de donnÃ©es CSV complets ne sont pas inclus dans ce dÃ©pÃ´t. Vous avez deux options pour exÃ©cuter ce script :

**Option A : DÃ©veloppement Local (avec les donnÃ©es complÃ¨tes)**

1.  Assurez-vous d'avoir tÃ©lÃ©chargÃ© les fichiers `application_train_rdy.csv` et `application_test_rdy.csv`.
2.  VÃ©rifiez que les chemins vers ces fichiers sont correctement configurÃ©s dans votre fichier `.env` (`TRAIN_DATA_FILE` et `TEST_DATA_FILE`).
3.  ExÃ©cutez la commande sans arguments.

```bash
poetry run python -m src.scripts.init_db
```

**Option B : Utilisation de DonnÃ©es Alternatives (ex: pour les tests)**

Vous pouvez spÃ©cifier le chemin vers d'autres fichiers de donnÃ©es (comme les petits fichiers de test `fixtures`) en utilisant des arguments. C'est la mÃ©thode utilisÃ©e en intÃ©gration continue.

```bash
poetry run python -m src.scripts.init_db \
  --train-file tests/fixtures/sample.train.csv \
  --test-file tests/fixtures/sample_test.csv
```

### 7. Lancer l'API FastAPI

Dans un premier terminal :

```bash
poetry run uvicorn src.api.main:app --reload
```
L'API sera accessible Ã  l'adresse `http://127.0.0.1:8000`.

### 8. Lancer le Dashboard Streamlit

Dans un second terminal :

```bash
poetry run streamlit run app.py
```
Le dashboard sera accessible Ã  l'adresse `http://localhost:8501`.

## âœ… Tests

Pour lancer la suite de tests automatisÃ©s, exÃ©cutez la commande suivante depuis la racine du projet :

```bash
poetry run pytest
```

## ğŸ”¬ Analyse de Performance

Cette section dÃ©crit les outils utilisÃ©s pour mesurer et analyser la performance de l'API. **Assurez-vous que le serveur de l'API est en cours d'exÃ©cution** avant de lancer ces scripts.

### Profiling de l'API (`cProfile`)

Le script `profile_api.py` utilise `cProfile` pour analyser les goulots d'Ã©tranglement de l'API. Il effectue plusieurs appels Ã  l'endpoint de prÃ©diction et mesure le temps passÃ© dans chaque fonction.

**ExÃ©cution :**
```bash
poetry run python -m src.scripts.profile_api
```

### Test de Charge (`Locust`)

Le script `locustfile.py` utilise Locust pour simuler une montÃ©e en charge et tester la robustesse de l'API sous la pression de plusieurs utilisateurs virtuels.

**ExÃ©cution :**
1.  **Lancez Locust :**
    ```bash
    poetry run python -m locust -f src/scripts/locustfile.py --host="[http://127.0.0.1:8000](http://127.0.0.1:8000)"
    ```
2.  **Ouvrez l'interface web de Locust** dans votre navigateur Ã  l'adresse `http://localhost:8089`.
3.  **Configurez et dÃ©marrez un test** en spÃ©cifiant le nombre d'utilisateurs et le taux d'apparition.
